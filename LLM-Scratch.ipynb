{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f76cc1",
   "metadata": {},
   "source": [
    "# LLM4 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ad72a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Libraries imported and device configured.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import os\n",
    "import collections # For BPE-like processing if extended\n",
    "import re          # For initial splitting\n",
    "\n",
    "# --- Device Configuration ---\n",
    "# Theory: Set the device (GPU 'cuda' if available, else CPU) for tensor operations.\n",
    "# This ensures models and data are processed efficiently on available hardware.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"Libraries imported and device configured.\")\n",
    "\n",
    "\n",
    "# ### OUTPUT ###\n",
    "# PyTorch version: 2.6.0+cu124\n",
    "# Using device: cuda\n",
    "# Libraries imported and device configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb9d789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training corpus defined (length: 593 characters).\n"
     ]
    }
   ],
   "source": [
    "# Define the raw text corpus for training\n",
    "corpus_raw = \"\"\"\n",
    "Alice was beginning to get very tired of sitting by her sister on the\n",
    "bank, and of having nothing to do: once or twice she had peeped into the\n",
    "book her sister was reading, but it had no pictures or conversations in\n",
    "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
    "conversation?'\n",
    "So she was considering in her own mind (as well as she could, for the\n",
    "hot day made her feel very sleepy and stupid), whether the pleasure\n",
    "of making a daisy-chain would be worth the trouble of getting up and\n",
    "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
    "close by her.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Training corpus defined (length: {len(corpus_raw)} characters).\")\n",
    "\n",
    "\n",
    "# ### OUTPUT ###\n",
    "# Training corpus defined (length: 593 characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8961ddcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created character vocabulary of size: 36\n",
      "Vocabulary: \n",
      " '(),-.:?ARSWabcdefghiklmnoprstuvwy\n"
     ]
    }
   ],
   "source": [
    "# Find all unique characters in the raw corpus\n",
    "chars = sorted(list(set(corpus_raw)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create character-to-integer mapping (encoding)\n",
    "char_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "\n",
    "# Create integer-to-character mapping (decoding)\n",
    "int_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "print(f\"Created character vocabulary of size: {vocab_size}\")\n",
    "print(f\"Vocabulary: {''.join(chars)}\")\n",
    "# Optional: Print mappings\n",
    "# print(f\"Char-to-Int mapping sample: {{k: char_to_int[k] for k in list(char_to_int)[:5]}}\")\n",
    "# print(f\"Int-to-Char mapping sample: {{k: int_to_char[k] for k in list(int_to_char)[:5]}}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5caf6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded corpus into a tensor of shape: torch.Size([593])\n"
     ]
    }
   ],
   "source": [
    "# Encode the entire corpus into a list of integer IDs\n",
    "encoded_corpus = [char_to_int[ch] for ch in corpus_raw]\n",
    "\n",
    "# Convert the list into a PyTorch tensor\n",
    "full_data_sequence = torch.tensor(encoded_corpus, dtype=torch.long, device=device)\n",
    "\n",
    "print(f\"Encoded corpus into a tensor of shape: {full_data_sequence.shape}\")\n",
    "# Optional: Display first few encoded IDs\n",
    "# print(f\"First 50 encoded token IDs: {full_data_sequence[:50].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c16ed593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Architecture Hyperparameters ---\n",
    "# vocab_size is already determined from the data\n",
    "d_model = 128         # Embedding dimension (reduced significantly)\n",
    "n_layers = 4          # Number of Transformer blocks (reduced)\n",
    "n_heads = 4           # Number of attention heads\n",
    "block_size = 64       # Maximum context length (sequence length)\n",
    "rms_norm_eps = 1e-5   # Epsilon for RMSNorm stability\n",
    "rope_theta = 10000.0  # Theta parameter for RoPE (reduced from Llama 4's 500k)\n",
    "\n",
    "# --- MoE Specific Hyperparameters ---\n",
    "num_local_experts = 4      # Number of experts per MoE layer (reduced from 16)\n",
    "num_experts_per_tok = 2   # Number of experts to route each token to (Top-K, reduced from 4?)\n",
    "intermediate_size_expert = d_model * 2  # Hidden dimension within each expert MLP (scaled down)\n",
    "intermediate_size_shared = d_model * 2  # Hidden dimension within the shared MLP (scaled down)\n",
    "\n",
    "# --- Attention Hyperparameters ---\n",
    "# d_k (dimension per head) will be derived from d_model and n_heads\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "learning_rate = 5e-4  # Learning rate\n",
    "batch_size = 16       # Number of sequences processed in parallel\n",
    "epochs = 3000         # Number of training iterations (adjust as needed)\n",
    "eval_interval = 300  # How often to print loss\n",
    "\n",
    "# --- Derived Hyperparameters ---\n",
    "assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "d_k = d_model // n_heads # Dimension of keys/queries/values per head\n",
    "expert_dim = intermediate_size_expert # Alias for clarity\n",
    "shared_expert_dim = intermediate_size_shared # Alias for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "848a0c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 529 overlapping input/target sequence pairs.\n",
      "Shape of train_x: torch.Size([529, 64])\n",
      "Shape of train_y: torch.Size([529, 64])\n"
     ]
    }
   ],
   "source": [
    "# Create lists to hold all possible input (x) and target (y) sequences\n",
    "all_x = []\n",
    "all_y = []\n",
    "\n",
    "# Iterate through the encoded corpus tensor to extract overlapping sequences\n",
    "num_total_tokens = len(full_data_sequence)\n",
    "for i in range(num_total_tokens - block_size):\n",
    "    # Extract the input sequence chunk\n",
    "    x_chunk = full_data_sequence[i : i + block_size]\n",
    "    # Extract the target sequence chunk (shifted one position right)\n",
    "    y_chunk = full_data_sequence[i + 1 : i + block_size + 1]\n",
    "    all_x.append(x_chunk)\n",
    "    all_y.append(y_chunk)\n",
    "\n",
    "# Stack the lists of tensors into single large tensors\n",
    "train_x = torch.stack(all_x)\n",
    "train_y = torch.stack(all_y)\n",
    "\n",
    "num_sequences_available = train_x.shape[0]\n",
    "print(f\"Created {num_sequences_available} overlapping input/target sequence pairs.\")\n",
    "print(f\"Shape of train_x: {train_x.shape}\") # Should be (num_sequences, block_size)\n",
    "print(f\"Shape of train_y: {train_y.shape}\") # Should be (num_sequences, block_size)\n",
    "\n",
    "# Optional: Verify device\n",
    "# print(f\"train_x is on device: {train_x.device}\") # May still be on CPU, move in batching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec8017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready for training. Will sample batches of size 16 randomly.\n",
      "Batches will be moved to device during the training loop.\n"
     ]
    }
   ],
   "source": [
    "# Check if we have enough sequences for the desired batch size\n",
    "if num_sequences_available < batch_size:\n",
    "    print(f\"Warning: Number of sequences ({num_sequences_available}) is less than batch size ({batch_size}). Adjusting batch size.\")\n",
    "    batch_size = num_sequences_available\n",
    "\n",
    "print(f\"Data ready for training. Will sample batches of size {batch_size} randomly.\")\n",
    "print(\"Batches will be moved to device during the training loop.\")\n",
    "# Example of how a batch would be selected in the loop:\n",
    "# indices = torch.randint(0, num_sequences_available, (batch_size,))\n",
    "# xb = train_x[indices].to(device)\n",
    "# yb = train_y[indices].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e1001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Token Embedding Layer:\n",
      "  Input Vocab Size: 36\n",
      "  Output Embedding Dim (d_model): 128\n",
      "  Weight shape: torch.Size([36, 128])\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize the token embedding table\n",
    "token_embedding_table = nn.Embedding(vocab_size, d_model).to(device)\n",
    "\n",
    "print(f\"Initialized Token Embedding Layer:\")\n",
    "print(f\"  Input Vocab Size: {vocab_size}\")\n",
    "print(f\"  Output Embedding Dim (d_model): {d_model}\")\n",
    "print(f\"  Weight shape: {token_embedding_table.weight.shape}\")\n",
    "print(f\"  Device: {token_embedding_table.weight.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbe3f304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputed RoPE inverse frequencies (inv_freq):\n",
      "  Shape: torch.Size([16])\n",
      "  Values (first 5): [1.0, 0.5623413324356079, 0.3162277638912201, 0.17782793939113617, 0.10000000149011612]\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Precompute the inverse frequencies for RoPE\n",
    "# Formula: 1.0 / (rope_theta ** (torch.arange(0, d_k, 2) / d_k))\n",
    "rope_freq_indices = torch.arange(0, d_k, 2, dtype=torch.float, device=device)\n",
    "inv_freq = 1.0 / (rope_theta ** (rope_freq_indices / d_k))\n",
    "\n",
    "print(\"Precomputed RoPE inverse frequencies (inv_freq):\")\n",
    "print(f\"  Shape: {inv_freq.shape}\") # Should be (d_k / 2,)\n",
    "print(f\"  Values (first 5): {inv_freq[:5].tolist()}\")\n",
    "print(f\"  Device: {inv_freq.device}\")\n",
    "# The 'freqs_cis' (complex numbers) will be computed in the forward pass using these inv_freq and position_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a719a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RMSNorm weights for 4 layers...\n",
      "  Initialized RMSNorm weights for Layer 1 (Input: torch.Size([128]), PostAttn: torch.Size([128]))\n",
      "  Initialized RMSNorm weights for Layer 2 (Input: torch.Size([128]), PostAttn: torch.Size([128]))\n",
      "  Initialized RMSNorm weights for Layer 3 (Input: torch.Size([128]), PostAttn: torch.Size([128]))\n",
      "  Initialized RMSNorm weights for Layer 4 (Input: torch.Size([128]), PostAttn: torch.Size([128]))\n",
      "Initialized Final RMSNorm weight, shape: torch.Size([128])\n",
      "RMSNorm weights initialized (as nn.Parameter). The normalization logic will be inline.\n"
     ]
    }
   ],
   "source": [
    "# Lists to store RMSNorm layer weights for each Transformer block\n",
    "rmsnorm_weights_input = []      # RMSNorm before MHA\n",
    "rmsnorm_weights_post_attn = []  # RMSNorm before MoE/FFN\n",
    "\n",
    "print(f\"Initializing RMSNorm weights for {n_layers} layers...\")\n",
    "for i in range(n_layers):\n",
    "    # RMSNorm weight for input to attention\n",
    "    # Initialize weight as torch.ones, similar to nn.LayerNorm's default gamma\n",
    "    weight_in = nn.Parameter(torch.ones(d_model, device=device))\n",
    "    rmsnorm_weights_input.append(weight_in)\n",
    "\n",
    "    # RMSNorm weight for input to MoE/FFN (post-attention)\n",
    "    weight_post = nn.Parameter(torch.ones(d_model, device=device))\n",
    "    rmsnorm_weights_post_attn.append(weight_post)\n",
    "    print(f\"  Initialized RMSNorm weights for Layer {i+1} (Input: {weight_in.shape}, PostAttn: {weight_post.shape})\")\n",
    "\n",
    "# Final RMSNorm before the output layer\n",
    "final_rmsnorm_weight = nn.Parameter(torch.ones(d_model, device=device))\n",
    "\n",
    "print(f\"Initialized Final RMSNorm weight, shape: {final_rmsnorm_weight.shape}\")\n",
    "print(\"RMSNorm weights initialized (as nn.Parameter). The normalization logic will be inline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0e8f398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Attention (MHA) linear layers for 4 layers...\n",
      "  Initialized MHA Linears for Layer 1 (QKV: torch.Size([384, 128]), Out: torch.Size([128, 128]))\n",
      "  Initialized MHA Linears for Layer 2 (QKV: torch.Size([384, 128]), Out: torch.Size([128, 128]))\n",
      "  Initialized MHA Linears for Layer 3 (QKV: torch.Size([384, 128]), Out: torch.Size([128, 128]))\n",
      "  Initialized MHA Linears for Layer 4 (QKV: torch.Size([384, 128]), Out: torch.Size([128, 128]))\n",
      "Attention (MHA) linear layers initialized.\n"
     ]
    }
   ],
   "source": [
    "# Lists to store Attention layers for each Transformer block\n",
    "mha_qkv_linears = []    # Combined Linear layer for Q, K, V projections\n",
    "mha_output_linears = [] # Output Linear layer for MHA\n",
    "\n",
    "print(f\"Initializing Attention (MHA) linear layers for {n_layers} layers...\")\n",
    "for i in range(n_layers):\n",
    "    # Combined QKV projection layer\n",
    "    # Bias is often False in large transformer QKV projections\n",
    "    qkv_linear = nn.Linear(d_model, 3 * d_model, bias=False).to(device)\n",
    "    mha_qkv_linears.append(qkv_linear)\n",
    "\n",
    "    # Output projection layer\n",
    "    # Bias is often False here too, but can be True\n",
    "    output_linear = nn.Linear(d_model, d_model, bias=False).to(device)\n",
    "    mha_output_linears.append(output_linear)\n",
    "    print(f\"  Initialized MHA Linears for Layer {i+1} (QKV: {qkv_linear.weight.shape}, Out: {output_linear.weight.shape})\")\n",
    "\n",
    "print(\"Attention (MHA) linear layers initialized.\")\n",
    "\n",
    "\n",
    "### OUTPUT ###\n",
    "# Initializing Attention (MHA) linear layers for 4 layers...\n",
    "#   Initialized MHA Linears for Layer 1 (QKV: torch.Size([384, 128]), Out: torch.Size([128, 128]))\n",
    "#   Initialized MHA Linears for Layer 2 (QKV: torch.Size([384, 128]), Out: torch.Size([128, 128]))\n",
    "#   Initialized MHA Linears for Layer 3 (QKV: torch.Size([384, 128]), Out: torch.Size([128, 128]))\n",
    "#   Initialized MHA Linears for Layer 4 (QKV: torch.Size([384, 128]), Out: torch.Size([128, 128]))\n",
    "# Attention (MHA) linear layers initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e00c0c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MoE and Shared MLP components for 4 layers...\n",
      "  Num Experts per layer: 4\n",
      "  Expert Dim: 256\n",
      "  Shared MLP Dim: 256\n",
      "  Initialized MoE components for Layer 1:\n",
      "    Router weights: torch.Size([4, 128])\n",
      "    Expert Gate/Up weights: torch.Size([4, 128, 512])\n",
      "    Expert Down weights: torch.Size([4, 256, 128])\n",
      "    Shared Gate weights: torch.Size([256, 128])\n",
      "    Shared Up weights: torch.Size([256, 128])\n",
      "    Shared Down weights: torch.Size([128, 256])\n",
      "  Initialized MoE components for Layer 2:\n",
      "    Router weights: torch.Size([4, 128])\n",
      "    Expert Gate/Up weights: torch.Size([4, 128, 512])\n",
      "    Expert Down weights: torch.Size([4, 256, 128])\n",
      "    Shared Gate weights: torch.Size([256, 128])\n",
      "    Shared Up weights: torch.Size([256, 128])\n",
      "    Shared Down weights: torch.Size([128, 256])\n",
      "  Initialized MoE components for Layer 3:\n",
      "    Router weights: torch.Size([4, 128])\n",
      "    Expert Gate/Up weights: torch.Size([4, 128, 512])\n",
      "    Expert Down weights: torch.Size([4, 256, 128])\n",
      "    Shared Gate weights: torch.Size([256, 128])\n",
      "    Shared Up weights: torch.Size([256, 128])\n",
      "    Shared Down weights: torch.Size([128, 256])\n",
      "  Initialized MoE components for Layer 4:\n",
      "    Router weights: torch.Size([4, 128])\n",
      "    Expert Gate/Up weights: torch.Size([4, 128, 512])\n",
      "    Expert Down weights: torch.Size([4, 256, 128])\n",
      "    Shared Gate weights: torch.Size([256, 128])\n",
      "    Shared Up weights: torch.Size([256, 128])\n",
      "    Shared Down weights: torch.Size([128, 256])\n",
      "MoE and Shared MLP components initialized.\n"
     ]
    }
   ],
   "source": [
    "# Lists to store MoE components for each layer\n",
    "moe_routers = []             # Router linear layers\n",
    "moe_expert_gate_up_proj = [] # Expert Gate/Up projection weights\n",
    "moe_expert_down_proj = []    # Expert Down projection weights\n",
    "shared_expert_gate_proj = [] # Shared Expert Gate projection\n",
    "shared_expert_up_proj = []   # Shared Expert Up projection\n",
    "shared_expert_down_proj = [] # Shared Expert Down projection\n",
    "\n",
    "print(f\"Initializing MoE and Shared MLP components for {n_layers} layers...\")\n",
    "print(f\"  Num Experts per layer: {num_local_experts}\")\n",
    "print(f\"  Expert Dim: {expert_dim}\")\n",
    "print(f\"  Shared MLP Dim: {shared_expert_dim}\")\n",
    "\n",
    "for i in range(n_layers):\n",
    "    # 1. Router\n",
    "    router_linear = nn.Linear(d_model, num_local_experts, bias=False).to(device)\n",
    "    moe_routers.append(router_linear)\n",
    "\n",
    "    # 2. Experts (Weights as Parameters)\n",
    "    # Gate/Up Projection Weight: (num_experts, d_model, 2 * expert_dim)\n",
    "    # Note: Combining Gate and Up projection into one weight matrix here\n",
    "    gate_up_w = nn.Parameter(torch.empty(num_local_experts, d_model, 2 * expert_dim, device=device))\n",
    "    nn.init.normal_(gate_up_w, mean=0.0, std=0.02) # Example initialization\n",
    "    moe_expert_gate_up_proj.append(gate_up_w)\n",
    "\n",
    "    # Down Projection Weight: (num_experts, expert_dim, d_model)\n",
    "    down_w = nn.Parameter(torch.empty(num_local_experts, expert_dim, d_model, device=device))\n",
    "    nn.init.normal_(down_w, mean=0.0, std=0.02) # Example initialization\n",
    "    moe_expert_down_proj.append(down_w)\n",
    "\n",
    "    # 3. Shared Expert (Standard MLP layers)\n",
    "    shared_gate = nn.Linear(d_model, shared_expert_dim, bias=False).to(device)\n",
    "    shared_up = nn.Linear(d_model, shared_expert_dim, bias=False).to(device)\n",
    "    shared_down = nn.Linear(shared_expert_dim, d_model, bias=False).to(device)\n",
    "    shared_expert_gate_proj.append(shared_gate)\n",
    "    shared_expert_up_proj.append(shared_up)\n",
    "    shared_expert_down_proj.append(shared_down)\n",
    "\n",
    "    print(f\"  Initialized MoE components for Layer {i+1}:\")\n",
    "    print(f\"    Router weights: {router_linear.weight.shape}\")\n",
    "    print(f\"    Expert Gate/Up weights: {gate_up_w.shape}\")\n",
    "    print(f\"    Expert Down weights: {down_w.shape}\")\n",
    "    print(f\"    Shared Gate weights: {shared_gate.weight.shape}\")\n",
    "    print(f\"    Shared Up weights: {shared_up.weight.shape}\")\n",
    "    print(f\"    Shared Down weights: {shared_down.weight.shape}\")\n",
    "\n",
    "print(\"MoE and Shared MLP components initialized.\")\n",
    "# Activation function (used inline)\n",
    "activation_fn = nn.SiLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e359e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Final Output Linear Layer:\n",
      "  Input Dim (d_model): 128\n",
      "  Output Dim (vocab_size): 36\n",
      "  Weight shape: torch.Size([36, 128])\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Final Linear Layer (language modeling head)\n",
    "output_linear_layer = nn.Linear(d_model, vocab_size, bias=False).to(device)\n",
    "\n",
    "print(f\"Initialized Final Output Linear Layer:\")\n",
    "print(f\"  Input Dim (d_model): {d_model}\")\n",
    "print(f\"  Output Dim (vocab_size): {vocab_size}\")\n",
    "print(f\"  Weight shape: {output_linear_layer.weight.shape}\")\n",
    "print(f\"  Device: {output_linear_layer.weight.device}\")\n",
    "\n",
    "\n",
    "### OUTPUT ###\n",
    "# Initialized Final Output Linear Layer:\n",
    "#   Input Dim (d_model): 128\n",
    "#   Output Dim (vocab_size): 36\n",
    "#   Weight shape: torch.Size([36, 128])\n",
    "#   Device: cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c568768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputed Causal Attention Mask:\n",
      "  Shape: torch.Size([1, 1, 64, 64])\n",
      "  Requires grad: False\n"
     ]
    }
   ],
   "source": [
    "# Create the lower triangular mask for causal self-attention\n",
    "# Values are 1 where attention is allowed, 0 where it's masked.\n",
    "# Shape: (1, 1, block_size, block_size) for broadcasting with (B, n_heads, T, T)\n",
    "causal_mask = torch.tril(torch.ones(block_size, block_size, device=device))\n",
    "causal_mask = causal_mask.view(1, 1, block_size, block_size)\n",
    "\n",
    "print(\"Precomputed Causal Attention Mask:\")\n",
    "print(f\"  Shape: {causal_mask.shape}\")\n",
    "print(f\"  Requires grad: {causal_mask.requires_grad}\")\n",
    "# Optional: Visualize the mask for a smaller block size\n",
    "# if block_size <= 8:\n",
    "#    print(causal_mask[0, 0].cpu().numpy())\n",
    "\n",
    "\n",
    "### OUTPUT ###\n",
    "# Precomputed Causal Attention Mask:\n",
    "#   Shape: torch.Size([1, 1, 64, 64])\n",
    "#   Requires grad: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7006b8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer Setup:\n",
      "  Optimizer: AdamW\n",
      "  Learning Rate: 0.0005\n",
      "  Managing 43 parameter groups/tensors.\n",
      "  Total Trainable Parameters: 2,240,640\n"
     ]
    }
   ],
   "source": [
    "# Gather all model parameters requiring gradients\n",
    "all_model_parameters = list(token_embedding_table.parameters())\n",
    "# Add RMSNorm weights\n",
    "all_model_parameters.extend(rmsnorm_weights_input)\n",
    "all_model_parameters.extend(rmsnorm_weights_post_attn)\n",
    "all_model_parameters.append(final_rmsnorm_weight)\n",
    "# Add Attention linear layer weights\n",
    "for i in range(n_layers):\n",
    "    all_model_parameters.extend(list(mha_qkv_linears[i].parameters()))\n",
    "    all_model_parameters.extend(list(mha_output_linears[i].parameters()))\n",
    "# Add MoE Router linear layer weights\n",
    "for i in range(n_layers):\n",
    "    all_model_parameters.extend(list(moe_routers[i].parameters()))\n",
    "# Add MoE Expert weights (already nn.Parameters)\n",
    "all_model_parameters.extend(moe_expert_gate_up_proj)\n",
    "all_model_parameters.extend(moe_expert_down_proj)\n",
    "# Add Shared Expert linear layer weights\n",
    "for i in range(n_layers):\n",
    "    all_model_parameters.extend(list(shared_expert_gate_proj[i].parameters()))\n",
    "    all_model_parameters.extend(list(shared_expert_up_proj[i].parameters()))\n",
    "    all_model_parameters.extend(list(shared_expert_down_proj[i].parameters()))\n",
    "# Add Final Output linear layer weights\n",
    "all_model_parameters.extend(list(output_linear_layer.parameters()))\n",
    "\n",
    "# Count total number of parameter tensors (groups)\n",
    "num_param_groups = len(all_model_parameters)\n",
    "# Count total number of individual parameters\n",
    "total_params = sum(p.numel() for p in all_model_parameters if p.requires_grad)\n",
    "\n",
    "# Define the AdamW optimizer\n",
    "optimizer = optim.AdamW(all_model_parameters, lr=learning_rate)\n",
    "\n",
    "print(\"Optimizer Setup:\")\n",
    "print(f\"  Optimizer: {type(optimizer).__name__}\")\n",
    "print(f\"  Learning Rate: {learning_rate}\")\n",
    "print(f\"  Managing {num_param_groups} parameter groups/tensors.\")\n",
    "print(f\"  Total Trainable Parameters: {total_params:,}\")\n",
    "\n",
    "\n",
    "\n",
    "#### OUTPUT ####\n",
    "# Optimizer Setup:\n",
    "#   Optimizer: AdamW\n",
    "#   Learning Rate: 0.0005\n",
    "#   Managing 43 parameter groups/tensors.\n",
    "#   Total Trainable Parameters: 2,240,640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69765597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886cff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training Loop for 3000 epochs ---\n",
      "  Epoch 1/3000, Loss: 3.7789\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Starting Training Loop for {epochs} epochs ---\")\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Sample a random batch of data\n",
    "    ix = torch.randint(num_sequences_available, (batch_size,))\n",
    "    xb = train_x[ix].to(device)\n",
    "    yb = train_y[ix].to(device)\n",
    "\n",
    "    # --- Forward Pass ---\n",
    "    B, T = xb.shape\n",
    "    token_embed = token_embedding_table(xb) # (B, T, d_model)\n",
    "\n",
    "    # Prepare RoPE frequencies for the current sequence length\n",
    "    position_ids = torch.arange(T, device=device).unsqueeze(0) # (1, T)\n",
    "    # Correct calculation for freqs_cis\n",
    "    t_indices = torch.arange(T, device=device)\n",
    "    freqs = 1.0 / (rope_theta ** (torch.arange(0, d_k, 2, device=device).float() / d_k)) # (d_k/2)\n",
    "    m_theta = torch.outer(t_indices, freqs).float() # (T, d_k/2)\n",
    "    freqs_cis = torch.polar(torch.ones_like(m_theta), m_theta) # (T, d_k/2) complex\n",
    "    freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(2) # (1, T, 1, d_k/2) for broadcasting\n",
    "\n",
    "\n",
    "    x = token_embed\n",
    "    for i in range(n_layers):\n",
    "        # Residual connection starts here\n",
    "        residual = x\n",
    "\n",
    "        # 1. RMSNorm before Attention\n",
    "        x_norm = (x.float() * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)) * rmsnorm_weights_input[i]\n",
    "\n",
    "        # 2. Multi-Head Attention\n",
    "        qkv = mha_qkv_linears[i](x_norm) # (B, T, 3 * d_model)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # Each is (B, T, d_model)\n",
    "\n",
    "        # Reshape for multi-head\n",
    "        q = q.view(B, T, n_heads, d_k) # (B, T, n_heads, d_k)\n",
    "        k = k.view(B, T, n_heads, d_k)\n",
    "        v = v.view(B, T, n_heads, d_k)\n",
    "\n",
    "        # Apply RoPE to Q and K\n",
    "        q = q.view(B, T, n_heads, d_k//2, 2)\n",
    "        k = k.view(B, T, n_heads, d_k//2, 2)\n",
    "        q_complex = torch.view_as_complex(q.float()) # (B, T, n_heads, d_k/2)\n",
    "        k_complex = torch.view_as_complex(k.float()) # (B, T, n_heads, d_k/2)\n",
    "\n",
    "        # Apply rotation based on position\n",
    "        # Ensure freqs_cis aligns with sequence length T\n",
    "        q_rotated_complex = q_complex * freqs_cis[:, :T] # freqs_cis needs proper shape (1, T, 1, d_k/2)\n",
    "        k_rotated_complex = k_complex * freqs_cis[:, :T]\n",
    "\n",
    "        q_rotated = torch.view_as_real(q_rotated_complex).view(B, T, n_heads, d_k)\n",
    "        k_rotated = torch.view_as_real(k_rotated_complex).view(B, T, n_heads, d_k)\n",
    "\n",
    "\n",
    "        # Transpose for attention calculation: (B, n_heads, T, d_k)\n",
    "        q_rotated = q_rotated.transpose(1, 2)\n",
    "        k_rotated = k_rotated.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        attn_scores = (q_rotated @ k_rotated.transpose(-2, -1)) * (d_k ** -0.5) # (B, n_heads, T, T)\n",
    "        # Apply causal mask\n",
    "        attn_scores = attn_scores.masked_fill(causal_mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(attn_scores, dim=-1) # (B, n_heads, T, T)\n",
    "        attn_output = attention_weights @ v # (B, n_heads, T, d_k)\n",
    "\n",
    "        # Reshape and project output\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, d_model) # (B, T, d_model)\n",
    "        attn_output = mha_output_linears[i](attn_output)\n",
    "\n",
    "        # Add attention output to residual\n",
    "        x = residual + attn_output\n",
    "\n",
    "        # 3. RMSNorm before MoE/FFN & Residual for MoE/FFN\n",
    "        residual_moe = x\n",
    "        x_norm_moe = (x.float() * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)) * rmsnorm_weights_post_attn[i]\n",
    "\n",
    "        # 4. MoE Block\n",
    "        router_logits = moe_routers[i](x_norm_moe) # (B, T, num_local_experts)\n",
    "        routing_weights, selected_experts = torch.topk(router_logits, num_experts_per_tok, dim=-1)\n",
    "        routing_weights = F.softmax(routing_weights, dim=-1).to(x_norm_moe.dtype) # Normalize scores per token\n",
    "\n",
    "        # Initialize final output tensor\n",
    "        final_hidden_state = torch.zeros_like(x_norm_moe)\n",
    "\n",
    "        # Flatten tokens and experts for batch processing\n",
    "        flat_x = x_norm_moe.view(-1, d_model)                     # (B*T, d_model)\n",
    "        flat_router_weights = routing_weights.view(-1, num_experts_per_tok) # (B*T, num_experts_per_tok)\n",
    "        flat_selected_experts = selected_experts.view(-1, num_experts_per_tok) # (B*T, num_experts_per_tok)\n",
    "\n",
    "        # Calculate expert outputs\n",
    "        expert_outputs_list = []\n",
    "        for k in range(num_experts_per_tok):\n",
    "            expert_idx = flat_selected_experts[:, k] # Indices of the k-th best expert for each token (B*T)\n",
    "            token_indices = torch.arange(flat_x.size(0), device=device)\n",
    "\n",
    "            # Get weights for the selected experts\n",
    "            gate_up_w_k = moe_expert_gate_up_proj[i][expert_idx] # (B*T, d_model, 2 * expert_dim)\n",
    "            down_w_k = moe_expert_down_proj[i][expert_idx]     # (B*T, expert_dim, d_model)\n",
    "\n",
    "            # Perform expert calculations using bmm\n",
    "            # Input needs shape (B*T, 1, d_model) for bmm with (B*T, d_model, 2*expert_dim)\n",
    "            expert_input_k = flat_x.unsqueeze(1) # (B*T, 1, d_model)\n",
    "            gate_up_out_k = torch.bmm(expert_input_k, gate_up_w_k) # (B*T, 1, 2 * expert_dim)\n",
    "\n",
    "            # Split gate and up projections\n",
    "            gate_k, up_k = gate_up_out_k.chunk(2, dim=-1) # Each (B*T, 1, expert_dim)\n",
    "\n",
    "            # Apply activation and gating\n",
    "            activated_up_k = activation_fn(gate_k) * up_k # (B*T, 1, expert_dim)\n",
    "\n",
    "            # Down projection\n",
    "            # Input needs shape (B*T, 1, expert_dim) for bmm with (B*T, expert_dim, d_model)\n",
    "            expert_output_k = torch.bmm(activated_up_k, down_w_k) # (B*T, 1, d_model)\n",
    "            expert_output_k = expert_output_k.squeeze(1) # (B*T, d_model)\n",
    "\n",
    "            # Weight the expert output\n",
    "            expert_output_weighted_k = expert_output_k * flat_router_weights[:, k].unsqueeze(1)\n",
    "            expert_outputs_list.append(expert_output_weighted_k)\n",
    "\n",
    "        # Sum the weighted outputs of the selected experts\n",
    "        moe_output = torch.stack(expert_outputs_list, dim=0).sum(dim=0) # Sum over num_experts_per_tok\n",
    "        moe_output = moe_output.view(B, T, d_model)\n",
    "\n",
    "        # 5. Shared Expert MLP (applied to the same x_norm_moe)\n",
    "        shared_gate_val = shared_expert_gate_proj[i](x_norm_moe)\n",
    "        shared_up_val = shared_expert_up_proj[i](x_norm_moe)\n",
    "        shared_output = shared_expert_down_proj[i](activation_fn(shared_gate_val) * shared_up_val)\n",
    "\n",
    "        # Add MoE output and Shared output to the residual\n",
    "        x = residual_moe + moe_output + shared_output\n",
    "\n",
    "    # --- Final Layer ---\n",
    "    # RMSNorm before final layer\n",
    "    x = (x.float() * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + rms_norm_eps)) * final_rmsnorm_weight\n",
    "    logits = output_linear_layer(x) # (B, T, vocab_size)\n",
    "\n",
    "    # --- Calculate Loss ---\n",
    "    # Reshape logits and targets for CrossEntropyLoss\n",
    "    loss = criterion(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
    "\n",
    "    # --- Backward Pass and Optimization ---\n",
    "    optimizer.zero_grad(set_to_none=True) # More efficient zeroing\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # --- Logging ---\n",
    "    losses.append(loss.item())\n",
    "    if epoch % eval_interval == 0 or epoch == epochs - 1:\n",
    "        print(f\"  Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"--- Training Loop Completed ---\")\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"Matplotlib not found, skipping loss plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f85446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Step 7: Text Generation ---\")\n",
    "\n",
    "# --- Generation Parameters ---\n",
    "seed_chars = \"Alice \" # Starting text prompt\n",
    "num_tokens_to_generate = 200 # How many new characters to generate\n",
    "print(f\"Seed text: '{seed_chars}'\")\n",
    "print(f\"Generating {num_tokens_to_generate} new tokens...\")\n",
    "\n",
    "# --- Prepare Initial Context ---\n",
    "# Convert seed characters to token IDs\n",
    "seed_ids = [char_to_int[ch] for ch in seed_chars if ch in char_to_int]\n",
    "# Create the initial context tensor (add batch dimension)\n",
    "generated_sequence = torch.tensor([seed_ids], dtype=torch.long, device=device)\n",
    "print(f\"Initial context shape: {generated_sequence.shape}\")\n",
    "\n",
    "# --- Set Model Components to Evaluation Mode ---\n",
    "# (Important if Dropout or BatchNorm were used, good practice anyway)\n",
    "token_embedding_table.eval()\n",
    "for i in range(n_layers):\n",
    "    # RMSNorm doesn't have eval mode, just use weights\n",
    "    mha_qkv_linears[i].eval()\n",
    "    mha_output_linears[i].eval()\n",
    "    moe_routers[i].eval()\n",
    "    # Expert weights (Parameters) don't have eval()\n",
    "    shared_expert_gate_proj[i].eval()\n",
    "    shared_expert_up_proj[i].eval()\n",
    "    shared_expert_down_proj[i].eval()\n",
    "output_linear_layer.eval()\n",
    "# Final RMSNorm weight doesn't have eval()\n",
    "print(\"Model components set to evaluation mode (where applicable).\")\n",
    "\n",
    "\n",
    "### OUTPUT ###\n",
    "# --- Step 7: Text Generation ---\n",
    "# Seed text: 'Alice '\n",
    "# Generating 200 new tokens...\n",
    "# Initial context shape: torch.Size([1, 6])\n",
    "# Model components set to evaluation mode (where applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting generation loop...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_tokens_to_generate):\n",
    "        # Ensure context doesn't exceed block_size\n",
    "        current_context = generated_sequence[:, -block_size:]\n",
    "        B_gen, T_gen = current_context.shape\n",
    "\n",
    "        # --- Forward pass (similar to training, but without loss calc) ---\n",
    "        token_embed_gen = token_embedding_table(current_context)\n",
    "\n",
    "        # Prepare RoPE frequencies for the current sequence length T_gen\n",
    "        t_indices_gen = torch.arange(T_gen, device=device)\n",
    "        freqs_gen = 1.0 / (rope_theta ** (torch.arange(0, d_k, 2, device=device).float() / d_k)) # (d_k/2)\n",
    "        m_theta_gen = torch.outer(t_indices_gen, freqs_gen).float() # (T_gen, d_k/2)\n",
    "        freqs_cis_gen = torch.polar(torch.ones_like(m_theta_gen), m_theta_gen) # (T_gen, d_k/2) complex\n",
    "        freqs_cis_gen = freqs_cis_gen.unsqueeze(0).unsqueeze(2) # (1, T_gen, 1, d_k/2)\n",
    "\n",
    "        x_gen = token_embed_gen\n",
    "        for i in range(n_layers):\n",
    "            residual_gen = x_gen\n",
    "            x_norm_gen = (x_gen.float() * torch.rsqrt(x_gen.pow(2).mean(-1, keepdim=True) + rms_norm_eps)) * rmsnorm_weights_input[i]\n",
    "\n",
    "            qkv_gen = mha_qkv_linears[i](x_norm_gen)\n",
    "            q_gen, k_gen, v_gen = qkv_gen.chunk(3, dim=-1)\n",
    "\n",
    "            q_gen = q_gen.view(B_gen, T_gen, n_heads, d_k)\n",
    "            k_gen = k_gen.view(B_gen, T_gen, n_heads, d_k)\n",
    "            v_gen = v_gen.view(B_gen, T_gen, n_heads, d_k)\n",
    "\n",
    "            # Apply RoPE\n",
    "            q_gen = q_gen.view(B_gen, T_gen, n_heads, d_k//2, 2)\n",
    "            k_gen = k_gen.view(B_gen, T_gen, n_heads, d_k//2, 2)\n",
    "            q_complex_gen = torch.view_as_complex(q_gen.float())\n",
    "            k_complex_gen = torch.view_as_complex(k_gen.float())\n",
    "            q_rotated_complex_gen = q_complex_gen * freqs_cis_gen # Use freqs_cis_gen\n",
    "            k_rotated_complex_gen = k_complex_gen * freqs_cis_gen # Use freqs_cis_gen\n",
    "            q_rotated_gen = torch.view_as_real(q_rotated_complex_gen).view(B_gen, T_gen, n_heads, d_k)\n",
    "            k_rotated_gen = torch.view_as_real(k_rotated_complex_gen).view(B_gen, T_gen, n_heads, d_k)\n",
    "\n",
    "            q_rotated_gen = q_rotated_gen.transpose(1, 2)\n",
    "            k_rotated_gen = k_rotated_gen.transpose(1, 2)\n",
    "            v_gen = v_gen.transpose(1, 2)\n",
    "\n",
    "            attn_scores_gen = (q_rotated_gen @ k_rotated_gen.transpose(-2, -1)) * (d_k ** -0.5)\n",
    "            attn_scores_gen = attn_scores_gen.masked_fill(causal_mask[:,:,:T_gen,:T_gen] == 0, float('-inf'))\n",
    "            attention_weights_gen = F.softmax(attn_scores_gen, dim=-1)\n",
    "            attn_output_gen = attention_weights_gen @ v_gen\n",
    "            attn_output_gen = attn_output_gen.transpose(1, 2).contiguous().view(B_gen, T_gen, d_model)\n",
    "            attn_output_gen = mha_output_linears[i](attn_output_gen)\n",
    "            x_gen = residual_gen + attn_output_gen\n",
    "\n",
    "            residual_moe_gen = x_gen\n",
    "            x_norm_moe_gen = (x_gen.float() * torch.rsqrt(x_gen.pow(2).mean(-1, keepdim=True) + rms_norm_eps)) * rmsnorm_weights_post_attn[i]\n",
    "\n",
    "            # MoE Block (simplified for generation context)\n",
    "            router_logits_gen = moe_routers[i](x_norm_moe_gen)\n",
    "            routing_weights_gen, selected_experts_gen = torch.topk(router_logits_gen, num_experts_per_tok, dim=-1)\n",
    "            routing_weights_gen = F.softmax(routing_weights_gen, dim=-1).to(x_norm_moe_gen.dtype)\n",
    "\n",
    "            final_hidden_state_gen = torch.zeros_like(x_norm_moe_gen)\n",
    "            flat_x_gen = x_norm_moe_gen.view(-1, d_model)\n",
    "            flat_router_weights_gen = routing_weights_gen.view(-1, num_experts_per_tok)\n",
    "            flat_selected_experts_gen = selected_experts_gen.view(-1, num_experts_per_tok)\n",
    "\n",
    "            expert_outputs_list_gen = []\n",
    "            for k in range(num_experts_per_tok):\n",
    "                 expert_idx_gen = flat_selected_experts_gen[:, k]\n",
    "                 gate_up_w_k_gen = moe_expert_gate_up_proj[i][expert_idx_gen]\n",
    "                 down_w_k_gen = moe_expert_down_proj[i][expert_idx_gen]\n",
    "                 expert_input_k_gen = flat_x_gen.unsqueeze(1)\n",
    "                 gate_up_out_k_gen = torch.bmm(expert_input_k_gen, gate_up_w_k_gen)\n",
    "                 gate_k_gen, up_k_gen = gate_up_out_k_gen.chunk(2, dim=-1)\n",
    "                 activated_up_k_gen = activation_fn(gate_k_gen) * up_k_gen\n",
    "                 expert_output_k_gen = torch.bmm(activated_up_k_gen, down_w_k_gen).squeeze(1)\n",
    "                 expert_output_weighted_k_gen = expert_output_k_gen * flat_router_weights_gen[:, k].unsqueeze(1)\n",
    "                 expert_outputs_list_gen.append(expert_output_weighted_k_gen)\n",
    "\n",
    "            moe_output_gen = torch.stack(expert_outputs_list_gen, dim=0).sum(dim=0)\n",
    "            moe_output_gen = moe_output_gen.view(B_gen, T_gen, d_model)\n",
    "\n",
    "            shared_gate_val_gen = shared_expert_gate_proj[i](x_norm_moe_gen)\n",
    "            shared_up_val_gen = shared_expert_up_proj[i](x_norm_moe_gen)\n",
    "            shared_output_gen = shared_expert_down_proj[i](activation_fn(shared_gate_val_gen) * shared_up_val_gen)\n",
    "\n",
    "            x_gen = residual_moe_gen + moe_output_gen + shared_output_gen\n",
    "\n",
    "        # Final Layer prediction\n",
    "        x_gen = (x_gen.float() * torch.rsqrt(x_gen.pow(2).mean(-1, keepdim=True) + rms_norm_eps)) * final_rmsnorm_weight\n",
    "        logits_gen = output_linear_layer(x_gen) # (B, T_gen, vocab_size)\n",
    "\n",
    "        # Focus only on the logits for the last token\n",
    "        logits_last = logits_gen[:, -1, :] # (B, vocab_size)\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(logits_last, dim=-1)\n",
    "\n",
    "        # Sample the next token ID from the probability distribution\n",
    "        next_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "        # Append the sampled token ID to the sequence\n",
    "        generated_sequence = torch.cat((generated_sequence, next_token), dim=1)\n",
    "\n",
    "print(\"...Generation loop finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the generated sequence for the first (and only) batch item\n",
    "final_generated_ids = generated_sequence[0].tolist()\n",
    "\n",
    "# Decode the list of IDs back into a string\n",
    "decoded_text = ''.join([int_to_char.get(id_val, '[UNK]') for id_val in final_generated_ids])\n",
    "\n",
    "print(\"\\n--- Final Generated Text ---\")\n",
    "print(decoded_text)\n",
    "\n",
    "\n",
    "### OUTPUT ###\n",
    "# --- Final Generated Text ---\n",
    "# Alice 'without pictures or\n",
    "# conversation?'\n",
    "# So she was considering in her own mind (as well as she could, for the\n",
    "# hot day made her feel very sleepy and stupid), whether the pleasure\n",
    "# of making a daisy-chain wo ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7fec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store the model (if it doesn't exist)\n",
    "save_dir = 'saved_models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'llama4_moe_model.pt')\n",
    "\n",
    "# Create a state dictionary manually collecting all components\n",
    "model_state = {\n",
    "    # Configuration\n",
    "    'config': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'd_model': d_model,\n",
    "        'n_layers': n_layers,\n",
    "        'n_heads': n_heads,\n",
    "        'block_size': block_size,\n",
    "        'rms_norm_eps': rms_norm_eps,\n",
    "        'rope_theta': rope_theta,\n",
    "        'num_local_experts': num_local_experts,\n",
    "        'num_experts_per_tok': num_experts_per_tok,\n",
    "        'intermediate_size_expert': intermediate_size_expert,\n",
    "        'intermediate_size_shared': intermediate_size_shared\n",
    "    },\n",
    "    # Tokenizer\n",
    "    'tokenizer': {\n",
    "        'char_to_int': char_to_int,\n",
    "        'int_to_char': int_to_char\n",
    "    },\n",
    "    # Model Parameters (State Dicts for nn.Modules, Tensors for nn.Parameters)\n",
    "    'token_embedding_table': token_embedding_table.state_dict(),\n",
    "    'rmsnorm_weights_input': [p.data for p in rmsnorm_weights_input], # Save tensor data\n",
    "    'rmsnorm_weights_post_attn': [p.data for p in rmsnorm_weights_post_attn], # Save tensor data\n",
    "    'final_rmsnorm_weight': final_rmsnorm_weight.data, # Save tensor data\n",
    "    'mha_qkv_linears': [l.state_dict() for l in mha_qkv_linears],\n",
    "    'mha_output_linears': [l.state_dict() for l in mha_output_linears],\n",
    "    'moe_routers': [r.state_dict() for r in moe_routers],\n",
    "    'moe_expert_gate_up_proj': [p.data for p in moe_expert_gate_up_proj], # Save tensor data\n",
    "    'moe_expert_down_proj': [p.data for p in moe_expert_down_proj], # Save tensor data\n",
    "    'shared_expert_gate_proj': [l.state_dict() for l in shared_expert_gate_proj],\n",
    "    'shared_expert_up_proj': [l.state_dict() for l in shared_expert_up_proj],\n",
    "    'shared_expert_down_proj': [l.state_dict() for l in shared_expert_down_proj],\n",
    "    'output_linear_layer': output_linear_layer.state_dict(),\n",
    "    # Note: RoPE inv_freq is not saved as it's derived from config\n",
    "}\n",
    "\n",
    "# Save the state dictionary\n",
    "torch.save(model_state, save_path)\n",
    "\n",
    "print(f\"Model state saved successfully to '{save_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
