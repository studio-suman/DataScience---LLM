{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "020110ab",
   "metadata": {},
   "source": [
    "# SimpleRAG with OpenAI and Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20107b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2855631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = pdfplumber.open(pdf_path)  # Use pdfplumber to open the PDF file\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page in mypdf.pages:\n",
    "        text = page.extract_text()  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf69dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e43e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE\"),  # Retrieve the base URL from environment variables  # Retrieve the API version from environment variables     \n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249daa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 5\n",
      "\n",
      "First text chunk:\n",
      "Chapter 1\n",
      "It is said that “In the land of Lama don’t be Gama”, when I heard the saying for\n",
      "the 1st time, one place came to my mind is that of Leh Ladakh. Lush Green\n",
      "Meadows, barren mountains, blueish lakes from Fables, Gompas dotting the\n",
      "landscape for miles. For whole year COVID-19 swept across the globe wiping away\n",
      "almost half the population, millions jobless and also similar holocaust engulfed\n",
      "India as well. I work at a reputed MNC as HR and settled in Kolkata with my wife\n",
      "and Dad. As COVID swept across India our company announced Work from Home\n",
      "there by retaining our most of our JOBs and employees, people at IT were really\n",
      "blessed to be a part of this, most of us had our jobs!! Like others we were also\n",
      "scared to go out of the house amidst the crisis. Until we got ourselves vaccinated\n",
      "with 2nd Dose, till that we literally locked ourselves in closed doors as it was\n",
      "recommended by Govt. Myself including my family started feeling claustrophobic of\n",
      "not able to go out meet other people an\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_path = r\"C:\\DataScience - LLM\\Story_of_a_Lifetime.pdf\"  # Path to the PDF file\n",
    "# Check if the PDF file exists\n",
    "\n",
    "# Extract text from the PDF file\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# Print the number of text chunks created\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# Print the first text chunk\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4b0ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ollama_embeddings(text, model=\"llama3.2\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using Ollama's local models.\n",
    "\n",
    "    Args:\n",
    "    text (str or List[str]): The input text(s) for which embeddings are to be created.\n",
    "    model (str): The Ollama model to use. Default is \"llama3\".\n",
    "\n",
    "    Returns:\n",
    "    list: List of embeddings for the input text(s).\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    \n",
    "    # Convert single string to list for consistent processing\n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    # Process each text chunk\n",
    "    for chunk in text:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"http://localhost:11434/api/embeddings\",\n",
    "                json={\"model\": model, \"prompt\": chunk}\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                embeddings.append(result['embedding'])\n",
    "            else:\n",
    "                print(f\"Error: API returned status code {response.status_code}\")\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error making request: {e}\")\n",
    "            \n",
    "    return embeddings\n",
    "\n",
    "# Example usage:\n",
    "# response = create_ollama_embeddings(text_chunks, model=\"llama2\")\n",
    "\n",
    "response = create_ollama_embeddings(text_chunks, model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d0857b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#response = np.array(response)  # Convert the response to a NumPy array for easier manipulation\n",
    "# Print the shape of the response array\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f811442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): The first vector.\n",
    "    vec2 (np.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity between the two vectors.\n",
    "    \"\"\"\n",
    "    # Compute the dot product of the two vectors and divide by the product of their norms\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74edd900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search on the text chunks using the given query and embeddings.\n",
    "\n",
    "    Args:\n",
    "    query (str): The query for the semantic search.\n",
    "    text_chunks (List[str]): A list of text chunks to search through.\n",
    "    embeddings (List[List[float]]): A list of embeddings for the text chunks.\n",
    "    k (int): The number of top relevant text chunks to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of the top k most relevant text chunks based on the query.\n",
    "    \"\"\"\n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_ollama_embeddings(query, model=\"llama3.2\")[0]\n",
    "    similarity_scores = []  # Initialize a list to store similarity scores\n",
    "\n",
    "    # Calculate similarity scores between the query embedding and each text chunk embedding\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        # Direct use of chunk_embedding as it's already a list of floats\n",
    "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding))\n",
    "        similarity_scores.append((i, similarity_score))\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Get the indices of the top k most similar text chunks\n",
    "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
    "    # Return the top k most relevant text chunks\n",
    "    return [text_chunks[index] for index in top_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a25589b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation data from a JSON file\n",
    "with open('val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the first query from the validation data\n",
    "#query = data[0]['question']\n",
    "query = \"What is the author talking about the place?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44cfd647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the author talking about the place?\n",
      "Context 1:\n",
      "from the Siachen\n",
      "Glacier in the Karakoram range to the north to the main Great Himalayas to the\n",
      "south. The eastern end, consisting of the uninhabited Aksai Chin plains, is claimedby the Indian Government as part of Ladakh, and has been under Chinese control\n",
      "since 1962.\n",
      "=====================================\n",
      "Context 2:\n",
      "Chapter 1\n",
      "It is said that “In the land of Lama don’t be Gama”, when I heard the saying for\n",
      "the 1st time, one place came to my mind is that of Leh Ladakh. Lush Green\n",
      "Meadows, barren mountains, blueish lakes from Fables, Gompas dotting the\n",
      "landscape for miles. For whole year COVID-19 swept across the globe wiping away\n",
      "almost half the population, millions jobless and also similar holocaust engulfed\n",
      "India as well. I work at a reputed MNC as HR and settled in Kolkata with my wife\n",
      "and Dad. As COVID swept across India our company announced Work from Home\n",
      "there by retaining our most of our JOBs and employees, people at IT were really\n",
      "blessed to be a part of this, most of us had our jobs!! Like others we were also\n",
      "scared to go out of the house amidst the crisis. Until we got ourselves vaccinated\n",
      "with 2nd Dose, till that we literally locked ourselves in closed doors as it was\n",
      "recommended by Govt. Myself including my family started feeling claustrophobic of\n",
      "not able to go out meet other people an\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# The response variable already contains the embeddings, pass it directly\n",
    "top_chunks = semantic_search(query, text_chunks, response, k=2)\n",
    "\n",
    "# Print the query\n",
    "print(\"Query:\", query)\n",
    "\n",
    "# Print the top 2 most relevant text chunks\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23cb6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the system prompt for the AI assistant\n",
    "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"llama3.2\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the AI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The model to be used for generating the response. Default is \"llama3.2\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the AI model.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Perform semantic search to get the top chunks\n",
    "top_chunks = semantic_search(query, text_chunks, response, k=2)\n",
    "\n",
    "# Create the user prompt based on the top chunks\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# Generate AI response\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d7ec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: I do not have enough information to answer that. The provided text does not mention a book, but rather two separate contexts that appear to be unrelated. Context 1 discusses the geographical location of Ladakh, while Context 2 describes the author's personal experience during the COVID-19 pandemic. Without more context or information about the book being referred to, it is impossible to determine its main theme.\n"
     ]
    }
   ],
   "source": [
    "ai_response.choices[0].message.content.strip()  # type: ignore # Extract the content of the AI's response\n",
    "# Print the AI's response\n",
    "print(\"AI Response:\", ai_response.choices[0].message.content.strip())    # type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
